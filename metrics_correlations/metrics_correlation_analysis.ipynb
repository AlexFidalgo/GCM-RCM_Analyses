{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6ce4ec",
   "metadata": {},
   "source": [
    "# Metrics Correlation Analysis: GCM-RCM Error Metrics\n",
    "\n",
    "**Objective**: Analyze correlations between error metrics computed for climate model (GCM-RCM) pairs.\n",
    "\n",
    "**Data Structure**: For each combination of `(region, gridpoint, physical_variable, model)`, we have multiple error metrics (ACC, d, KGE, etc.). We compute correlations between these metrics to identify redundancy and orthogonal error facets.\n",
    "\n",
    "**Output**: Scatter plot matrix (pairplot), correlation heatmaps, and statistical summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510eb8fd",
   "metadata": {},
   "source": [
    "## 1. Project Setup & Configuration\n",
    "\n",
    "This section defines the structure and user-configurable parameters for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78911641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import os\n",
    "OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c03af9",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "**Programmer**: Edit the following parameters to customize your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2607e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions: ['FR']\n",
      "Physical Variables: ['ppt']\n",
      "Metric Abbreviations: ['H10 (MAHE)', 'd', 'dr', 'NED', 'MV', 'KGE (2009)']\n",
      "Correlation Method: pearson\n"
     ]
    }
   ],
   "source": [
    "# Available regions. Set to None or empty list to include ALL regions. Analyze only France region: ['FR']\n",
    "REGIONS = ['FR']  \n",
    "\n",
    "# Physical variables: 'ppt' (precipitation) or 'tas' (temperature). Set to None to include both. Only precipitation: ['ppt']\n",
    "PHYSICAL_VARIABLES = ['ppt']\n",
    "\n",
    "# Metric abbreviations to include in the correlation analysis. None = all metrics; or specify like ['ACC', 'd', 'KGE (2009)', 'BM']\n",
    "METRIC_ABBREVIATIONS = ['H10 (MAHE)', 'd', 'dr', 'NED', 'MV', 'KGE (2009)']\n",
    "\n",
    "# Correlation method: 'pearson', 'spearman', or 'kendall'\n",
    "CORRELATION_METHOD = 'pearson'\n",
    "\n",
    "print(f\"Regions: {REGIONS if REGIONS else 'ALL'}\")\n",
    "print(f\"Physical Variables: {PHYSICAL_VARIABLES if PHYSICAL_VARIABLES else 'ALL'}\")\n",
    "print(f\"Metric Abbreviations: {METRIC_ABBREVIATIONS if METRIC_ABBREVIATIONS else 'ALL'}\")\n",
    "print(f\"Correlation Method: {CORRELATION_METHOD}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c263f41",
   "metadata": {},
   "source": [
    "## 2. Load Data from Database\n",
    "\n",
    "We'll query the error and metrics tables from the database (as described in the SQL schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76d9a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to PostgreSQL database\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "\n",
    "from utils import get_db_connection\n",
    "\n",
    "try:\n",
    "    engine = get_db_connection()\n",
    "    conn = engine.connect()\n",
    "    print(f\"✓ Connected to PostgreSQL database\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error connecting to database: {e}\")\n",
    "    print(\"  Make sure DB_NAME, DB_USER, DB_PASSWORD, DB_HOST, DB_PORT environment variables are set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22aec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded: 329478 rows\n",
      "  Columns: ['region', 'gridpoint', 'physical_variable', 'model', 'rcm_id', 'gcm_id', 'metric_id', 'metric_name', 'mat_vector']\n",
      "\n",
      "First few rows:\n",
      "  region  gridpoint physical_variable  model  rcm_id  gcm_id  metric_id  \\\n",
      "0     FR          2               ppt     18       1       3         95   \n",
      "1     FR          2               ppt     19       2       3         95   \n",
      "2     FR          2               ppt     40       6       4         95   \n",
      "3     FR          2               ppt     35       2       3         95   \n",
      "4     FR          2               ppt     46      11       5         95   \n",
      "\n",
      "  metric_name  mat_vector  \n",
      "0          dr    0.354677  \n",
      "1          dr    0.343277  \n",
      "2          dr    0.351538  \n",
      "3          dr    0.383483  \n",
      "4          dr    0.293005  \n"
     ]
    }
   ],
   "source": [
    "where_clauses = []\n",
    "\n",
    "if REGIONS is not None and len(REGIONS) > 0:\n",
    "    regions_str = \"', '\".join(REGIONS)\n",
    "    where_clauses.append(f\"error.region IN ('{regions_str}')\")\n",
    "\n",
    "if PHYSICAL_VARIABLES is not None and len(PHYSICAL_VARIABLES) > 0:\n",
    "    vars_str = \"', '\".join(PHYSICAL_VARIABLES)\n",
    "    where_clauses.append(f\"error.physical_variable IN ('{vars_str}')\")\n",
    "\n",
    "if METRIC_ABBREVIATIONS is not None and len(METRIC_ABBREVIATIONS) > 0:\n",
    "    metrics_str = \"', '\".join(METRIC_ABBREVIATIONS)\n",
    "    where_clauses.append(f\"metrics.metric_name IN ('{metrics_str}')\")\n",
    "\n",
    "where_clause = \" AND \".join(where_clauses)\n",
    "if where_clause:\n",
    "    where_clause = f\"WHERE {where_clause}\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    error.region,\n",
    "    error.gridpoint,\n",
    "    error.physical_variable,\n",
    "    error.model,\n",
    "    error.rcm_id,\n",
    "    error.gcm_id,\n",
    "    error.metric_id,\n",
    "    metrics.metric_name,\n",
    "    error.mat_vector\n",
    "FROM \n",
    "    error \n",
    "LEFT JOIN \n",
    "    metrics ON metrics.id = error.metric_id\n",
    "{where_clause}\n",
    "\"\"\"\n",
    "\n",
    "df_raw = pd.read_sql_query(query, conn)\n",
    "print(f\"✓ Data loaded: {len(df_raw)} rows\")\n",
    "print(f\"  Columns: {list(df_raw.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_raw.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0abb16",
   "metadata": {},
   "source": [
    "## 3. Data Filtering & Preprocessing\n",
    "\n",
    "Apply user-defined filters to select the region(s), physical variable(s), and metric(s) of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2758a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data retrieved (pre-filtered by database query):\n",
      "  Total rows: 329478\n",
      "  Regions: ['FR']\n",
      "  Physical variables: ['ppt']\n",
      "  Unique metrics: 6\n",
      "\n",
      "Missing values:\n",
      "region               0\n",
      "gridpoint            0\n",
      "physical_variable    0\n",
      "model                0\n",
      "rcm_id               0\n",
      "gcm_id               0\n",
      "metric_id            0\n",
      "metric_name          0\n",
      "mat_vector           0\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (329478, 9)\n",
      "Metrics: ['H10 (MAHE)', 'KGE (2009)', 'MV', 'NED', 'd', 'dr']\n",
      "Unique models: 89\n",
      "Unique gridpoints: 617\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "print(f\"✓ Data retrieved (pre-filtered by database query):\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Regions: {sorted(df['region'].unique())}\")\n",
    "print(f\"  Physical variables: {sorted(df['physical_variable'].unique())}\")\n",
    "print(f\"  Unique metrics: {df['metric_name'].nunique()}\")\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Metrics: {sorted(df['metric_name'].unique())}\")\n",
    "print(f\"Unique models: {df['model'].nunique()}\")\n",
    "print(f\"Unique gridpoints: {df['gridpoint'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f50888",
   "metadata": {},
   "source": [
    "## 4. Create Pivot Table for Correlation Analysis\n",
    "\n",
    "Transform the data so that each row represents a unique `(region, gridpoint, physical_variable, model)` grouping,  \n",
    "and each column is a metric. This structure allows us to compute correlations between metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98c2b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pivot table created:\n",
      "  Shape: (54913, 6)\n",
      "  Rows (unique group_keys): 54913\n",
      "  Columns (metrics): 6\n",
      "\n",
      "Metrics in pivot table:\n",
      "['H10 (MAHE)', 'KGE (2009)', 'MV', 'NED', 'd', 'dr']\n",
      "\n",
      "First few rows:\n",
      "metric_name    H10 (MAHE)  KGE (2009)        MV        NED         d        dr\n",
      "group_key                                                                     \n",
      "FR_100_ppt_1     1.242379   -0.092011  2.478668  21.851604  0.379031  0.371351\n",
      "FR_100_ppt_10    0.935961   -0.012109  1.197039  18.082803  0.415196  0.213969\n",
      "FR_100_ppt_11    0.861331    0.050020  1.104480  17.909459  0.435181  0.286322\n",
      "FR_100_ppt_12    0.849339    0.092962  1.096309  17.605076  0.447742  0.339210\n",
      "FR_100_ppt_13    0.942766    0.032100  1.449987  19.549915  0.413416  0.239227\n",
      "\n",
      "Missing values per metric:\n",
      "metric_name\n",
      "H10 (MAHE)    0\n",
      "KGE (2009)    0\n",
      "MV            0\n",
      "NED           0\n",
      "d             0\n",
      "dr            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a grouping key for (region, gridpoint, physical_variable, model)\n",
    "df['group_key'] = df.apply(\n",
    "    lambda row: f\"{row['region']}_{row['gridpoint']}_{row['physical_variable']}_{row['model']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Pivot: rows are group_keys, columns are metric names, values are error values\n",
    "pivot_df = df.pivot_table(\n",
    "    index='group_key',\n",
    "    columns='metric_name',\n",
    "    values='mat_vector',\n",
    "    aggfunc='first'  # Should be unique per group_key, but just in case\n",
    ")\n",
    "\n",
    "print(f\"✓ Pivot table created:\")\n",
    "print(f\"  Shape: {pivot_df.shape}\")\n",
    "print(f\"  Rows (unique group_keys): {pivot_df.shape[0]}\")\n",
    "print(f\"  Columns (metrics): {pivot_df.shape[1]}\")\n",
    "print(f\"\\nMetrics in pivot table:\")\n",
    "print(pivot_df.columns.tolist())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(pivot_df.head())\n",
    "\n",
    "print(f\"\\nMissing values per metric:\")\n",
    "print(pivot_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e89be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Removed rows with missing values:\n",
      "  Original: 54913 rows\n",
      "  After dropna: 54913 rows\n",
      "  Rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "pivot_df_clean = pivot_df.dropna()\n",
    "\n",
    "print(f\"✓ Removed rows with missing values:\")\n",
    "print(f\"  Original: {pivot_df.shape[0]} rows\")\n",
    "print(f\"  After dropna: {pivot_df_clean.shape[0]} rows\")\n",
    "print(f\"  Rows removed: {pivot_df.shape[0] - pivot_df_clean.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98effbe0",
   "metadata": {},
   "source": [
    "## 5. Compute Pairwise Correlations\n",
    "\n",
    "Calculate correlation coefficients between all pairs of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6d77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Correlation matrix computed (pearson):\n",
      "  Shape: (6, 6)\n",
      "\n",
      "Correlation Matrix:\n",
      "metric_name  H10 (MAHE)  KGE (2009)        MV       NED         d        dr\n",
      "metric_name                                                                \n",
      "H10 (MAHE)     1.000000   -0.206456  0.796779  0.735813  0.032381 -0.224065\n",
      "KGE (2009)    -0.206456    1.000000  0.301979  0.207025  0.817592  0.875730\n",
      "MV             0.796779    0.301979  1.000000  0.898409  0.324015  0.334772\n",
      "NED            0.735813    0.207025  0.898409  1.000000  0.172221  0.316493\n",
      "d              0.032381    0.817592  0.324015  0.172221  1.000000  0.600129\n",
      "dr            -0.224065    0.875730  0.334772  0.316493  0.600129  1.000000\n"
     ]
    }
   ],
   "source": [
    "corr_matrix = pivot_df_clean.corr(method=CORRELATION_METHOD)\n",
    "\n",
    "print(f\"✓ Correlation matrix computed ({CORRELATION_METHOD}):\")\n",
    "print(f\"  Shape: {corr_matrix.shape}\")\n",
    "print(f\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17950586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_corr_with_pvalues(df, method='pearson'):\n",
    "    \"\"\"Compute pairwise correlations and p-values.\"\"\"\n",
    "    cols = df.columns\n",
    "    n_cols = len(cols)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        for j in range(i + 1, n_cols):\n",
    "            metric1 = cols[i]\n",
    "            metric2 = cols[j]\n",
    "            \n",
    "            if method == 'pearson':\n",
    "                corr, pval = pearsonr(df[metric1], df[metric2])\n",
    "            elif method == 'spearman':\n",
    "                corr, pval = spearmanr(df[metric1], df[metric2])\n",
    "            elif method == 'kendall':\n",
    "                corr, pval = kendalltau(df[metric1], df[metric2])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "            results.append({\n",
    "                'Metric 1': metric1,\n",
    "                'Metric 2': metric2,\n",
    "                'Correlation': corr,\n",
    "                'P-value': pval,\n",
    "                'N': len(df)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92426573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Pairwise correlations (15 pairs):\n",
      "  Metric 1   Metric 2  Correlation      P-value     N\n",
      "        MV        NED     0.898409 0.000000e+00 54913\n",
      "KGE (2009)         dr     0.875730 0.000000e+00 54913\n",
      "KGE (2009)          d     0.817592 0.000000e+00 54913\n",
      "H10 (MAHE)         MV     0.796779 0.000000e+00 54913\n",
      "H10 (MAHE)        NED     0.735813 0.000000e+00 54913\n",
      "         d         dr     0.600129 0.000000e+00 54913\n",
      "        MV         dr     0.334772 0.000000e+00 54913\n",
      "        MV          d     0.324015 0.000000e+00 54913\n",
      "       NED         dr     0.316493 0.000000e+00 54913\n",
      "KGE (2009)         MV     0.301979 0.000000e+00 54913\n",
      "H10 (MAHE)         dr    -0.224065 0.000000e+00 54913\n",
      "KGE (2009)        NED     0.207025 0.000000e+00 54913\n",
      "H10 (MAHE) KGE (2009)    -0.206456 0.000000e+00 54913\n",
      "       NED          d     0.172221 0.000000e+00 54913\n",
      "H10 (MAHE)          d     0.032381 3.201464e-14 54913\n"
     ]
    }
   ],
   "source": [
    "pairwise_df = pairwise_corr_with_pvalues(pivot_df_clean, method=CORRELATION_METHOD)\n",
    "\n",
    "pairwise_df['Abs_Corr'] = pairwise_df['Correlation'].abs()\n",
    "pairwise_df = pairwise_df.sort_values('Abs_Corr', ascending=False).drop('Abs_Corr', axis=1)\n",
    "\n",
    "print(f\"\\n✓ Pairwise correlations ({len(pairwise_df)} pairs):\")\n",
    "print(pairwise_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95b482",
   "metadata": {},
   "source": [
    "- $H_0$: zero correlation  \n",
    "- $H_1$: non-zero correlation\n",
    "\n",
    "This **p-value** is the two-sided probability of observing a correlation at least as extreme as the measured one under the null hypothesis.\n",
    "\n",
    "Assumptions\n",
    "- **Pearson**: assumes a linear relationship and (approximately) bivariate normality.  \n",
    "- **Spearman** / **Kendall**: rank-based (nonparametric), less sensitive to outliers or non-normality.\n",
    "\n",
    "Can also be seen as association:\n",
    "- $H_0$: The population correlation is zero (no association)\n",
    "- $H_1$: The population correlation is non-zero (association exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ed689",
   "metadata": {},
   "source": [
    "## 6. Scatter Plot Matrix (Pairplot)\n",
    "\n",
    "Generate a pairplot to visualize relationships between metrics (similar to the attached image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairplot\n",
    "plt.figure(figsize=(16, 16))\n",
    "g = sns.pairplot(\n",
    "    pivot_df_clean,\n",
    "    diag_kind='hist',\n",
    "    plot_kws={'alpha': 0.6, 's': 30},\n",
    "    diag_kws={'bins': 30, 'edgecolor': 'k', 'alpha': 0.7}\n",
    ")\n",
    "\n",
    "# Add correlation coefficients to upper triangle\n",
    "def add_correlation_annotations(g, corr_matrix):\n",
    "    \"\"\"Add correlation values to upper triangle of pairplot.\"\"\"\n",
    "    for i, ax_row in enumerate(g.axes):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            if i < j:  # Upper triangle\n",
    "                col_i = g.data.columns[i]\n",
    "                col_j = g.data.columns[j]\n",
    "                corr_val = corr_matrix.loc[col_i, col_j]\n",
    "                ax.text(\n",
    "                    0.5, 0.5,\n",
    "                    f'{corr_val:.2f}***' if abs(corr_val) > 0.7 else \n",
    "                    f'{corr_val:.2f}**' if abs(corr_val) > 0.5 else\n",
    "                    f'{corr_val:.2f}',\n",
    "                    transform=ax.transAxes,\n",
    "                    ha='center', va='center',\n",
    "                    fontsize=12, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "                )\n",
    "                ax.set_title('')\n",
    "\n",
    "add_correlation_annotations(g, corr_matrix)\n",
    "\n",
    "plt.suptitle(\n",
    "    f'Metrics Correlation Analysis - Scatter Plot Matrix\\n'\n",
    "    f'Region(s): {REGIONS if REGIONS else \"ALL\"} | '\n",
    "    f'Variable(s): {PHYSICAL_VARIABLES if PHYSICAL_VARIABLES else \"ALL\"} | '\n",
    "    f'Method: {CORRELATION_METHOD}',\n",
    "    fontsize=14, y=1.00\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "output_file = f\"{OUTPUT_DIR}/pairplot_{CORRELATION_METHOD}.png\"\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Pairplot saved to: {output_file}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91294687",
   "metadata": {},
   "source": [
    "## 7. Correlation Heatmap\n",
    "\n",
    "Visualize the correlation matrix as a heatmap with hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f584db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap with hierarchical clustering\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.clustermap(\n",
    "    corr_matrix,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    vmin=-1, vmax=1,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cbar_kws={'label': f'{CORRELATION_METHOD.capitalize()} Correlation'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "\n",
    "plt.suptitle(\n",
    "    f'Hierarchical Correlation Heatmap ({CORRELATION_METHOD})\\n'\n",
    "    f'Region(s): {REGIONS if REGIONS else \"ALL\"} | Variable(s): {PHYSICAL_VARIABLES if PHYSICAL_VARIABLES else \"ALL\"}',\n",
    "    fontsize=14, y=0.98\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "output_file = f\"{OUTPUT_DIR}/heatmap_clustered_{CORRELATION_METHOD}.png\"\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Heatmap saved to: {output_file}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d421b59",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "Save correlation matrices and pairwise correlations to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save correlation matrix to CSV\n",
    "corr_matrix.to_csv(f\"{OUTPUT_DIR}/correlation_matrix_{CORRELATION_METHOD}.csv\")\n",
    "print(f\"✓ Correlation matrix saved to: {OUTPUT_DIR}/correlation_matrix_{CORRELATION_METHOD}.csv\")\n",
    "\n",
    "# Save pairwise correlations to CSV\n",
    "pairwise_df.to_csv(f\"{OUTPUT_DIR}/pairwise_correlations_{CORRELATION_METHOD}.csv\", index=False)\n",
    "print(f\"✓ Pairwise correlations saved to: {OUTPUT_DIR}/pairwise_correlations_{CORRELATION_METHOD}.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Metric': corr_matrix.columns,\n",
    "    'Mean': pivot_df_clean.mean(),\n",
    "    'Std': pivot_df_clean.std(),\n",
    "    'Min': pivot_df_clean.min(),\n",
    "    'Max': pivot_df_clean.max(),\n",
    "})\n",
    "summary_stats.to_csv(f\"{OUTPUT_DIR}/metrics_summary_statistics.csv\", index=False)\n",
    "print(f\"✓ Summary statistics saved to: {OUTPUT_DIR}/metrics_summary_statistics.csv\")\n",
    "\n",
    "# Save analysis metadata\n",
    "metadata = {\n",
    "    'Analysis Date': pd.Timestamp.now(),\n",
    "    'Correlation Method': CORRELATION_METHOD,\n",
    "    'Regions': str(REGIONS),\n",
    "    'Physical Variables': str(PHYSICAL_VARIABLES),\n",
    "    'Metric Abbreviations': str(METRIC_ABBREVIATIONS),\n",
    "    'Number of Rows': pivot_df_clean.shape[0],\n",
    "    'Number of Metrics': pivot_df_clean.shape[1],\n",
    "    'Metrics': ', '.join(pivot_df_clean.columns)\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame(list(metadata.items()), columns=['Parameter', 'Value'])\n",
    "metadata_df.to_csv(f\"{OUTPUT_DIR}/analysis_metadata.csv\", index=False)\n",
    "print(f\"✓ Analysis metadata saved to: {OUTPUT_DIR}/analysis_metadata.csv\")\n",
    "\n",
    "print(f\"\\n✓ All results exported to: {OUTPUT_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6a6e4",
   "metadata": {},
   "source": [
    "## 9. Summary & Insights\n",
    "\n",
    "Summary of the correlation analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"  Total rows analyzed: {pivot_df_clean.shape[0]}\")\n",
    "print(f\"  Number of metrics: {pivot_df_clean.shape[1]}\")\n",
    "print(f\"  Correlation method: {CORRELATION_METHOD}\")\n",
    "\n",
    "print(f\"\\nMetrics included:\")\n",
    "for i, metric in enumerate(pivot_df_clean.columns, 1):\n",
    "    print(f\"  {i}. {metric}\")\n",
    "\n",
    "print(f\"\\nTop 10 Strongest Correlations (by absolute value):\")\n",
    "top_corr = pairwise_df.head(10)\n",
    "for idx, row in top_corr.iterrows():\n",
    "    sig_marker = \"***\" if row['P-value'] < 0.001 else \"**\" if row['P-value'] < 0.01 else \"*\" if row['P-value'] < 0.05 else \"ns\"\n",
    "    print(f\"  {row['Metric 1']:15} <-> {row['Metric 2']:15} : {row['Correlation']:7.3f} {sig_marker}\")\n",
    "\n",
    "print(f\"\\nTop 10 Weakest Correlations (by absolute value):\")\n",
    "weak_corr = pairwise_df.tail(10)\n",
    "for idx, row in weak_corr.iterrows():\n",
    "    sig_marker = \"***\" if row['P-value'] < 0.001 else \"**\" if row['P-value'] < 0.01 else \"*\" if row['P-value'] < 0.05 else \"ns\"\n",
    "    print(f\"  {row['Metric 1']:15} <-> {row['Metric 2']:15} : {row['Correlation']:7.3f} {sig_marker}\")\n",
    "\n",
    "print(f\"\\nSignificance levels (α = 0.05):\")\n",
    "sig_count = (pairwise_df['P-value'] < 0.05).sum()\n",
    "print(f\"  Significant correlations: {sig_count} / {len(pairwise_df)} ({100*sig_count/len(pairwise_df):.1f}%)\")\n",
    "\n",
    "print(\"\\nNote: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
